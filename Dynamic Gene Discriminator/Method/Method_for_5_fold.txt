# Setting the Working Directory
setwd("##")

# Install and Load Necessary Libraries
install.packages("dplyr")
install.packages("xgboost")
install.packages("caret")
install.packages("openxlsx")
library(xgboost)
library(data.table)
library(readxl)
library(openxlsx)
library(caret)
library(pROC)
library(dplyr)

# Read the Training Data
train_data <- read_excel("train_set.xlsx")
train_data <- as.data.table(train_data)

train_data[, Class_numeric := as.numeric(factor(Class, levels = c("NonDy", "Dy"))) - 1]

# Define Features and Labels
features <- train_data[, .(Length, `GC content`, Exon, AS, NonAS, TE, NonTE, Conserved, NonConserved, Coding, NonCoding)]
label <- factor(train_data$Class, levels = c("NonDy", "Dy"))
train_df <- data.frame(features, label = label)

train_control <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# Define a Grid for Narrow Hyperparameter Tuning
tune_grid <- expand.grid(
  nrounds = c(1000), 
  eta = c(0.001, 0.01, 0.1), 
  max_depth = c(3, 5, 7), 
  gamma = c(0.3, 0.5, 0.7),  
  colsample_bytree = c(0.3, 0.5, 0.7), 
  min_child_weight = c(1, 3, 5), 
  subsample = c(0.3, 0.5, 0.7)  
)

xgb_tune_model <- train(
  label ~ ., 
  data = train_df, 
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = tune_grid,
  metric = "ROC"
)

print(xgb_tune_model)
best_params <- xgb_tune_model$bestTune
cv_results <- xgb_tune_model$results
write.xlsx(cv_results, "hyperparameter_search_results.xlsx")
